import numpy as np 
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing 
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix
import itertools 

np.set_printoptions(threshold=np.inf)

# creating confusion matrix for evaluating performance 
def plotCf(a,b,t):
    cf = confusion_matrix(a,b)
    plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')
    plt.colorbar()
    plt.title(t)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    tick_marks = np.arange(len(set(a))) # Length of classes
    class_labels = ['0','1']
    plt.xticks(tick_marks,class_labels)
    plt.yticks(tick_marks,class_labels)
    thresh = cf.max() / 2
    for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])): 
        plt.text(j,i,format(cf[i,j], 'd'), horizontalalignment='center', color='white' if cf[i,j] > thresh else 'black')
    plt.show()



# 2 layer neural network from scratch
def Sigmoid(Z): 
    return 1/(1+np.exp(-Z)) # squash output layer predictions between 0 and 1 

def Relu(Z): 
    return np.maximum(0,Z) # apply nonlinearity to hidden layers & avoid vanishing gradients

def dRelu2(dZ,Z): # backprop, sets gradient to 0 where Z >= 0 (non positive), keeps existing gradient (dZ) unchanged for positive outputs
    dZ[Z <= 0] = 0
    return dZ

def dRelu(x): # backprop, 0 for non-positve, 1 for positive inputs
    x[x<=0] = 0
    x[x>0] = 1 
    return x

def dSigmoid(Z): # backprop, calculates gradient for sigmoid activated layers
    s = 1/(1+np.exp(-Z))
    dZ = s * (1-s)
    return dZ



# setting up neural network structure
class dlnet: 
    def __init__(self,x,y): 
        self.debug = 0 
        self.X=x
        self.Y=y
        self.Yh=np.zeros((1,self.Y.shape[1]))
        self.L=2 # 2 layers, one hidden one output
        self.dims = [9, 15, 1] # number of neurons
        self.param = {} # storing the weights and biases for each layer
        self.ch = {} # cache layers during forward propagation
        self.grad = {} # gradient storage
        self.loss = [] # loss values
        self.lr = 0.003 # low learning rate to prevent overshooting
        self.sam = self.Y.shape[1]
        self.threshold = 0.5



    def nInit(self): 
        # random initialization for the weights to prevent symmetry, that way they do not learn the same features
        np.random.seed(1) 
        self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) # generates random matrix (15,9), then divides weights by a square root of the number of input neurons (Xavier Initialization)
        self.param['b1'] = np.zeros((self.dims[1], 1)) # initalizes biases for the hidden layer as matrix of zeros
        self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) # initializing weights between hidden layer and output layer
        self.param['b2'] = np.zeros((self.dims[2], 1)) # inializes the biases for the output layer as a zero matrix
        return



    def forward(self): 
        Z1 = self.param['W1'].dot(self.X) + self.param['b1'] # linear transformation of the hidden layer
        A1 = Relu(Z1) # activation of the hidden layer
        self.ch['Z1'],self.ch['A1'] = Z1,A1 # stores intermediate results from forward propagation in a cache dictionary for backpropagation



        # same thing but for output layer
        Z2 = self.param['W2'].dot(A1) + self.param['b2'] 
        A2 = Sigmoid(Z2) 
        self.ch['Z2'], self.ch['A2'] = Z2, A2
        self.Yh = A2 # storing predictions
        loss = self.nloss(A2) # computing loss (binary cross entropy loss)
        return self.Yh, loss



    def nloss(self, Yh): 
        # compute binary cross entropy loss by averaging the negative log-likelihood of predictions (Yh) as true labels (Y)
        loss = (1./self.sam) * (-np.dot(self.Y,np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))
        return loss



    def backward(self): 
        dLoss_Yh = -(np.divide(self.Y, self.Yh) - np.divide(1 - self.Y, 1 - self.Yh)) # compute gradient loss with respect to output
        dLoss_Z2 = dLoss_Yh * dSigmoid(self.ch['Z2']) # gradient for the output layer
        dLoss_A1 = np.dot(self.param["W2"].T,dLoss_Z2) # gradient with respect to hidden layer activations
        
        # gradient for output layer parameters
        dLoss_W2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2,self.ch['A1'].T) 
        dLoss_b2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2, np.ones([dLoss_Z2.shape[1],1]))

        dLoss_Z1 = dLoss_A1 * dRelu(self.ch['Z1']) # gradient for hidden layer
        dLoss_A0 = np.dot(self.param["W1"].T, dLoss_Z1) # gradient with respect to input layer activations

        # gradients for hidden layer parameters
        dLoss_W1 = 1./self.X.shape[1] * np.dot(dLoss_Z1,self.X.T)
        dLoss_b1 = 1./self.X.shape[1] * np.dot(dLoss_Z1, np.ones([dLoss_Z1.shape[1],1]))  
        
        # update paramters via gradient descent
        self.param["W1"] = self.param["W1"] - self.lr * dLoss_W1
        self.param["b1"] = self.param["b1"] - self.lr * dLoss_b1
        self.param["W2"] = self.param["W2"] - self.lr * dLoss_W2
        self.param["b2"] = self.param["b2"] - self.lr * dLoss_b2
        return




    def pred(self,x,y):
        self.X = x
        self.Y = y
        comp = np.zeros((1,x.shape[1])) # initialize prediction array
        pred, loss = self.forward() # perform forward propagation

        for i in range(0, pred.shape[1]):
            if pred[0,i] > self.threshold: comp[0,i] = 1 # convert probabilities to binary
            else: comp[0,i] = 0

        print("Acc: " + str(np.sum((comp == y)/x.shape[1]))) # calculate accuracy
        return comp

    def gd(self,X,Y,iter=3000): 
        np.random.seed(1) 
        self.nInit() 
        self.loss = []
        
        for i in range(0, iter): 
            Yh, loss = self.forward()
            self.backward()
            
            if i % 500 == 0:
                print("Cost after iteration %i: %f" %(i, loss))
                self.loss.append(float(np.squeeze(loss)))

        if len(self.loss) > 0:
            plt.figure(figsize=(10,6))
            plt.plot(range(0, iter, 500), self.loss, 'b-')
            plt.ylabel('Loss')
            plt.xlabel('Iterations')
            plt.title("Lr =" + str(self.lr))
            plt.grid(True)
            plt.show()
        return



# preprocessing the data
df = pd.read_csv(r"C:\Users\merit\source\repos\transfer cancer data to csv\transfer cancer data to csv\wisconsin-cancer-dataset.csv")



# Handle class mapping (benign = 0, malignant = 1)
df['Class'] = df['Class'].replace({2: 0, 4: 1}) 



# Remove the ID column since it's not relevant for prediction
df = df.drop('ID', axis=1)



# Handle missing values ('?') in Bare_Nuclei column
df['Bare_Nuclei'] = df['Bare_Nuclei'].replace('?', np.nan)
df = df.dropna()



# Convert Bare_Nuclei to float type
df['Bare_Nuclei'] = df['Bare_Nuclei'].astype(float)



# Scale the features using MinMaxScaler
scaler = MinMaxScaler()
feature_columns = df.columns[:-1]  # All columns except Class
df[feature_columns] = scaler.fit_transform(df[feature_columns])



# Split into training features (x) and target (y)
x = df.iloc[:500, :-1].values.transpose()
y = df.iloc[:500, -1:].values.transpose()



# Create validation set
xval = df.iloc[501:, :-1].values.transpose()
yval = df.iloc[501:, -1:].values.transpose()



# debugging print statements
print("Dataset shape:", df.shape)
print("Number of missing values:", df.isnull().sum().sum())
print("Feature ranges:")
print(df.describe())
print("\nTraining set shapes:")
print("x shape:", x.shape)
print("y shape:", y.shape)
print("\nValidation set shapes:")
print("xval shape:", xval.shape)
print("yval shape:", yval.shape)



# Print shapes to verify data splitting
print(df.shape, x.shape, y.shape, xval.shape, yval.shape)



# Initialize and train the neural network
nn = dlnet(x,y)
nn.lr = 0.03
nn.dims = [9, 15, 1]
nn.gd(x, y, iter=67000)



# Make predictions
pred_train = nn.pred(x, y)
pred_test = nn.pred(xval, yval)



# Set classification threshold
nn.threshold = 0.5



# Generate confusion matrix for training set
nn.X, nn.Y = x, y
target = np.around(np.squeeze(y), decimals=0).astype(np.int64)
predicted = np.around(np.squeeze(nn.pred(x,y)), decimals=0).astype(np.int64)
plotCf(target, predicted, 'Cf Training Set')


# Generate confusion matrix for validation set
nn.X, nn.Y = xval, yval
target = np.around(np.squeeze(yval), decimals=0).astype(np.int64)
predicted = np.around(np.squeeze(nn.pred(xval,yval)), decimals=0).astype(np.int64)
plotCf(target, predicted, 'Cf Validation Set')
